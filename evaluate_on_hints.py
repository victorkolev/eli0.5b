import json
from litellm import completion
from tqdm import tqdm
import os

# --- Configuration ---
# IMPORTANT: Make sure this points to the file generated by the MODIFIED hint script
# which includes the 'final_answer_gt' field.
INPUT_DATASET_PATH = "omnimath_100_with_hints_v2.jsonl" # Or your actual v2 filename
SMALL_LLM_MODEL = "ollama/qwen3:0.6b" # Your desired smaller model
OLLAMA_API_BASE = "http://localhost:11434"
OUTPUT_RESULTS_PATH = f"{SMALL_LLM_MODEL.replace('/', '_')}_eval_with_hints_results.json"

# --- Helper function for normalization ---
def normalize_answer(text):
    if text is None: return ""
    text = str(text).lower().strip()
    # Remove common prefixes
    prefixes = ["the final answer is", "the answer is", "answer:", "solution:"]
    for p in prefixes:
        if text.startswith(p):
            text = text[len(p):].strip()
    # Remove common LaTeX box commands, dollar signs, etc.
    text = text.replace("\\boxed{", "").replace("}", "")
    text = text.replace("$", "")
    # Remove periods at the end if it's just a number/simple expression
    if text.endswith('.'):
        text = text[:-1]
    # Remove all whitespace characters (spaces, tabs, newlines) for a stricter comparison
    # text = "".join(text.split()) # Uncomment for very strict whitespace insensitive comparison
    return text

# --- Load the dataset ---
dataset_with_hints = []
try:
    with open(INPUT_DATASET_PATH, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f): # Added line_num for better error reporting
            try:
                dataset_with_hints.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Skipping malformed line {line_num+1} in {INPUT_DATASET_PATH}: {e} - Line: {line.strip()}")
    print(f"Loaded {len(dataset_with_hints)} items from {INPUT_DATASET_PATH}")
except FileNotFoundError:
    print(f"Error: Input dataset file not found at {INPUT_DATASET_PATH}")
    exit()
except Exception as e:
    print(f"Error loading dataset: {e}")
    exit()

if not dataset_with_hints:
    print("No data loaded. Exiting.")
    exit()

results = []
correct_count = 0
error_count = 0

print(f"\n--- Evaluating model: {SMALL_LLM_MODEL} with hints ---")

for i, example in enumerate(tqdm(dataset_with_hints, desc="Evaluating with Hints")):
    question = example.get("question")
    hint = example.get("hint")
    # --- CORRECTED GROUND TRUTH LOADING ---
    concise_ground_truth = example.get("final_answer_gt") # Use the field with the concise GT
    detailed_solution_reference = example.get("detailed_solution") # For reference if needed
    # --- END OF CORRECTION ---

    if not all([question, hint, concise_ground_truth]): # Check concise_ground_truth
        print(f"\nSkipping example {i+1} due to missing data: Q={bool(question)}, H={bool(hint)}, ConciseGT={bool(concise_ground_truth)}")
        error_count += 1
        results.append({
            "original_question": question,
            "hint": hint,
            "ground_truth_concise": concise_ground_truth,
            "detailed_solution_reference": detailed_solution_reference,
            "model_response": "[ERROR] Missing input data for this item.",
            "correct": False,
            "error_message": "Missing input data for this item."
        })
        continue

    # Normalize the concise ground truth
    normalized_gt = normalize_answer(concise_ground_truth)

    prompt_with_hint = f"""
Here is a math question:
{question}

Here is a hint to help you solve it:
{hint}

Based on the question and the hint, please provide only the final answer to the question.
Your final answer:
"""
    model_response_text = ""
    normalized_model_response = ""
    error_message = None
    is_correct = False

    try:
        response = completion(
            model=SMALL_LLM_MODEL,
            api_base=OLLAMA_API_BASE,
            messages=[{"role": "user", "content": prompt_with_hint}],
            # temperature=0.1, # Consider very low temp for "final answer" tasks
            # timeout=60
        )
        if response.choices and response.choices[0].message and response.choices[0].message.content:
            model_response_text = response.choices[0].message.content.strip()
            normalized_model_response = normalize_answer(model_response_text)
        else:
            model_response_text = "[ERROR] Model returned empty or malformed response."
            error_message = "Model returned empty or malformed response."
            print(f"\nWarning: Empty/malformed response for question {i+1}. Full response: {response.model_dump_json(indent=2) if hasattr(response, 'model_dump_json') else response}")
    except Exception as e:
        model_response_text = f"[ERROR] API call failed: {str(e)}"
        error_message = str(e)
        error_count +=1
        print(f"\nError processing question {i+1}: {e}")

    # Evaluation logic using normalized answers
    if normalized_gt and normalized_model_response and not error_message:
        if normalized_gt == normalized_model_response:
            is_correct = True
        # Optional: a more lenient check if exact match fails,
        # but be cautious as it can lead to false positives.
        # elif normalized_gt in normalized_model_response:
        #     print(f"    Lenient (substring) match for Q{i+1}: GT='{normalized_gt}', Model='{normalized_model_response}'")
        #     is_correct = True # Decide if you want to count this
    else:
        if not normalized_gt and concise_ground_truth: # GT was present but became empty after normalization
            print(f"    Warning Q{i+1}: Ground truth became empty after normalization. Original GT: '{concise_ground_truth}'")
        if not normalized_model_response and model_response_text and not error_message: # Model response became empty
            print(f"    Warning Q{i+1}: Model response became empty after normalization. Original Response: '{model_response_text}'")


    if is_correct:
        correct_count += 1

    results.append({
        "original_question": question,
        "hint": hint,
        "prompt_sent_to_model": prompt_with_hint,
        "ground_truth_concise": concise_ground_truth,
        "model_response": model_response_text,
        "normalized_gt": normalized_gt,
        "normalized_model_response": normalized_model_response,
        "correct": is_correct,
        "error_message": error_message,
        "detailed_solution_reference": detailed_solution_reference
    })

# --- Summary and Saving ---
print(f"\n--- Evaluation Summary for {SMALL_LLM_MODEL} (with hints) ---")
# ... (summary print logic remains the same) ...
if len(dataset_with_hints) > error_count:
    valid_evaluations = len(dataset_with_hints) - error_count # Number of items model actually processed
    accuracy = (correct_count / valid_evaluations) * 100 if valid_evaluations > 0 else 0
    print(f"Total questions loaded: {len(dataset_with_hints)}")
    print(f"Successfully processed by model (attempted API call): {valid_evaluations}")
    print(f"Correctly answered: {correct_count}")
    print(f"Accuracy (on successfully processed): {accuracy:.2f}% ({correct_count}/{valid_evaluations})")
    print(f"API/Processing/Missing Data Errors: {error_count}")
else:
    print("No questions were successfully evaluated.")
    print(f"API/Processing/Missing Data Errors: {error_count}")

# Save results
try:
    with open(OUTPUT_RESULTS_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nFull results saved to: {OUTPUT_RESULTS_PATH}")
except IOError as e:
    print(f"Error saving results to {OUTPUT_RESULTS_PATH}: {e}")

# Optional: view incorrect ones
print("\nðŸ” Incorrectly answered or errored questions (first 5):")
# ... (incorrect display logic remains the same) ...
incorrect_shown = 0
for r_idx, r_val in enumerate(results): # Use r_idx, r_val to avoid conflict with outer loop's i
    if not r_val["correct"]:
        print(f"\n--- Item {r_idx+1} (Incorrect/Error) ---")
        print(f"Q: {r_val['original_question']}")
        # print(f"Hint: {r_val['hint']}") # Can be very long
        print(f"GT (Concise): {r_val['ground_truth_concise']}")
        print(f"Normalized GT: '{r_val['normalized_gt']}'")
        print(f"Model ({SMALL_LLM_MODEL}): {r_val['model_response']}")
        print(f"Normalized Model: '{r_val['normalized_model_response']}'")
        if r_val.get('error_message'):
            print(f"Error: {r_val['error_message']}")
        incorrect_shown += 1
        if incorrect_shown >= 5:
            break
if incorrect_shown == 0 and valid_evaluations > 0:
    print("No incorrect answers or errors found in the displayed sample (or all were correct)!")
elif valid_evaluations == 0:
    print("No questions were processed to check for correctness.")